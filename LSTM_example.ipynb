{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-#\n",
    "import string\n",
    "\n",
    "class ModelConfig(object):\n",
    "    def __init__(self):\n",
    "        self.num_unrollings = 10 # 每条数据的字符串长度\n",
    "        self.batch_size = 64 # 每一批数据的个数\n",
    "        self.vocabulary_size = len(string.ascii_lowercase) + 1 # 定义出现字符串的个数(一共有26个英文字母和一个空格)\n",
    "        self.summary_frequency = 100 # 生成样本的频率\n",
    "        self.num_steps = 7001 # 训练步数\n",
    "        self.num_nodes = 64 # 隐含层个数\n",
    "\n",
    "config = ModelConfig()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-#\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "class LoadData(object):\n",
    "    def __init__(self, valid_size=1000):\n",
    "        self.text = self._read_data()\n",
    "        self.valid_text = self.text[:valid_size]\n",
    "        self.train_text = self.text[valid_size:]\n",
    "\n",
    "    def _read_data(self, filename='text8.zip'):\n",
    "        with zipfile.ZipFile(filename) as f:\n",
    "            # 获取当中的一个文件\n",
    "            name = f.namelist()[0]\n",
    "            print('file name : %s ' % name)\n",
    "            data = tf.compat.as_str(f.read(name))\n",
    "        return data\n",
    "\n",
    "\n",
    "def char2id(char):\n",
    "    # 将字母转换成id\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"Unexpencted character: %s \" % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    # 将id转换成字母\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "    # 根据传入的概率向量得到相应的词\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    # 用于测试得到的batches是否符合原来的字符组合\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-#\n",
    "import numpy as np\n",
    "#from handleData import char2id\n",
    "#from config import config\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        # 每个串之间的间距\n",
    "        segment = self._text_size // self._batch_size\n",
    "        # 记录每个串当前的位置\n",
    "        self._cursor =[ offset * segment for offset in range(self._batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"\n",
    "        从当前数据的游标位置生成单一批数据，一个batch的大小为(batch, 27)\n",
    "        \"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, config.vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            # 生成one-hot向量\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        # 因为这里加入了上一批数据的最后一个字符，所以当前这批\n",
    "        # 数据每串长度为num_unrollings + 1\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-#\n",
    "import tensorflow as tf\n",
    "#from config import config\n",
    "\n",
    "class LSTM_Cell(object):\n",
    "\n",
    "    def __init__(self, train_data, train_label, num_nodes=64):\n",
    "        with tf.variable_scope(\"input\", initializer=tf.truncated_normal_initializer(-0.1, 0.1)) as input_layer:\n",
    "            self.ix, self.im, self.ib = self._generate_w_b(\n",
    "                x_weights_size=[config.vocabulary_size, num_nodes],\n",
    "                m_weights_size=[num_nodes, num_nodes],\n",
    "                biases_size=[1, num_nodes])\n",
    "        with tf.variable_scope(\"memory\", initializer=tf.truncated_normal_initializer(-0.1, 0.1)) as update_layer:\n",
    "            self.cx, self.cm, self.cb = self._generate_w_b(\n",
    "                x_weights_size=[config.vocabulary_size, num_nodes],\n",
    "                m_weights_size=[num_nodes, num_nodes],\n",
    "                biases_size=[1, num_nodes])\n",
    "        with tf.variable_scope(\"forget\", initializer=tf.truncated_normal_initializer(-0.1, 0.1)) as forget_layer:\n",
    "            self.fx, self.fm, self.fb = self._generate_w_b(\n",
    "                x_weights_size=[config.vocabulary_size, num_nodes],\n",
    "                m_weights_size=[num_nodes, num_nodes],\n",
    "                biases_size=[1, num_nodes])\n",
    "        with tf.variable_scope(\"output\", initializer=tf.truncated_normal_initializer(-0.1, 0.1)) as output_layer:\n",
    "            self.ox, self.om, self.ob = self._generate_w_b(\n",
    "                x_weights_size=[config.vocabulary_size, num_nodes],\n",
    "                m_weights_size=[num_nodes, num_nodes],\n",
    "                biases_size=[1, num_nodes])\n",
    "\n",
    "        self.w = tf.Variable(tf.truncated_normal([num_nodes, config.vocabulary_size], -0.1, 0.1))\n",
    "        self.b = tf.Variable(tf.zeros([config.vocabulary_size]))\n",
    "\n",
    "        self.saved_output = tf.Variable(tf.zeros([config.batch_size, num_nodes]), trainable=False)\n",
    "        self.saved_state = tf.Variable(tf.zeros([config.batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.train_label = train_label\n",
    "\n",
    "    def _generate_w_b(self, x_weights_size, m_weights_size, biases_size):\n",
    "        x_w = tf.get_variable(\"x_weights\", x_weights_size)\n",
    "        m_w = tf.get_variable(\"m_weigths\", m_weights_size)\n",
    "        b = tf.get_variable(\"biases\", config.batch_size, initializer=tf.constant_initializer(0.0))\n",
    "        return x_w, m_w, b\n",
    "\n",
    "    def _run(self, input, output, state):\n",
    "        forget_gate = tf.sigmoid(tf.matmul(input, self.fx) + tf.matmul(output, self.fm) + self.fb)\n",
    "        input_gate = tf.sigmoid(tf.matmul(input, self.ix) + tf.matmul(output, self.im) + self.ib)\n",
    "        update = tf.matmul(input, self.cx) + tf.matmul(output, self.cm) + self.cb\n",
    "        state = state * forget_gate + tf.tanh(update) * input_gate\n",
    "        output_gate = tf.sigmoid(tf.matmul(input, self.ox) + tf.matmul(output, self.om) + self.ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    def loss_func(self):\n",
    "        outputs = list()\n",
    "        output = self.saved_output\n",
    "        state = self.saved_state\n",
    "        for i in self.train_data:\n",
    "            output, state = self._run(i, output, state)\n",
    "            outputs.append(output)\n",
    "        # finnaly, the length of outputs is num_unrollings\n",
    "        with tf.control_dependencies([\n",
    "                self.saved_output.assign(output),\n",
    "                self.saved_state.assign(state)\n",
    "            ]):\n",
    "            # concat(0, outputs) to concat the list of output on the dim 0\n",
    "            # the length of outputs is batch_size\n",
    "            logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), self.w, self.b)\n",
    "            # the label should fix the size of ouputs\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=tf.concat(self.train_label, 0),\n",
    "                    logits=logits))\n",
    "            train_prediction = tf.nn.softmax(logits)\n",
    "        return logits, loss, train_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-#\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "#from config import config\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    # 随机概率分布采样\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    # 随机采样生成one-hot向量\n",
    "    p = np.zeros(shape=[1, config.vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    # 生成随机概率向量,向量大小为1*27\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, config.vocabulary_size])\n",
    "    return b / np.sum(b, 1)[:, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name : text8 \n",
      "WARNING:tensorflow:From <ipython-input-10-31cfc9e80834>:72: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Initialized....\n",
      "Average loss at step 0: 3.288911 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.81\n",
      "================================================================================\n",
      "adpft ns tnertnanip oed rjencwb pxhscpylzbeici jnytilinrslwlsk m enverktnfna kak\n",
      "qoskrprsyfeg q lqigix itvn  imiruerysnxiwico pqimtnkkxiiyeaoa mrpzregodaeapzz co\n",
      "d hceplabivtays qseqcpwaecuizt nyaqrrseiefmuoc tetlvdzze jpebmmwegi tnt r gi sse\n",
      "eaiw  rl  ukgntaf i  psxe  ikt   kjjh ub yiptcxsyvszeqyylm  nwirajgwdrge kpmbcz \n",
      "rohlvnerv  ud njiiijso aa uflxstgkjnaunblj axi nrfd stihey pdwpitedita qeiii g p\n",
      "================================================================================\n",
      "Average loss at step 100: 2.613408 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.59\n",
      "Average loss at step 200: 2.265571 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Average loss at step 300: 2.106622 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Average loss at step 400: 1.999016 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Average loss at step 500: 1.937007 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Average loss at step 600: 1.908777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Average loss at step 700: 1.857806 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Average loss at step 800: 1.817074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Average loss at step 900: 1.827782 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Average loss at step 1000: 1.823782 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "================================================================================\n",
      "hild the exporten the gurgurias ital lay term the madsch murba s acheving aurchi\n",
      "en whio hind his repalinked as teryer how incan the armanean fickopple the may o\n",
      "p us zero five whett stentizy of the lort am dahlave be at dation the presuriste\n",
      "y fib esuat jestritet to be of the rulds no cound the setter or canah mots as th\n",
      "pate the ped s brage the charm eppeamal grovication but sound that it appested t\n",
      "================================================================================\n",
      "Average loss at step 1100: 1.773880 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Average loss at step 1200: 1.750393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Average loss at step 1300: 1.730956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Average loss at step 1400: 1.738203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Average loss at step 1500: 1.734953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Average loss at step 1600: 1.739266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Average loss at step 1700: 1.709077 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Average loss at step 1800: 1.669714 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Average loss at step 1900: 1.643875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Average loss at step 2000: 1.690911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      "usts ressuencepy acceptions seoned compets of many sarger roman constate onevera\n",
      "econn in the requasiy torning of britced x applated and progence s jormer nother\n",
      "amentina lith de m set but albutba the comperts was impersive in that bodosa sma\n",
      "mm exsiention laenn for charo mlenies in daemfloyer reaved has marry rowary popu\n",
      "ic ewaldhyalism allong unities s one not levelal vidanjer audda came of grown ch\n",
      "================================================================================\n",
      "Average loss at step 2100: 1.683955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Average loss at step 2200: 1.676120 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Average loss at step 2300: 1.644693 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 2400: 1.652434 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Average loss at step 2500: 1.675544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Average loss at step 2600: 1.648133 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Average loss at step 2700: 1.655213 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Average loss at step 2800: 1.643727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Average loss at step 2900: 1.648264 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Average loss at step 3000: 1.648664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "================================================================================\n",
      "ch one nine zero ypaninn government in marahment norsts deriquie the sucperse is\n",
      "vabs one nine eight five three form lond droversemon agreatreals in oned the ano\n",
      "t salch of the mad baud enexprede sole have prosation ber france which rering an\n",
      "on elector methngest one nine nine nite it emproeth into to the protocally slate\n",
      "fers also use are and of good yorking his l d a some as or footnetermaled collec\n",
      "================================================================================\n",
      "Average loss at step 3100: 1.624712 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Average loss at step 3200: 1.640713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Average loss at step 3300: 1.630784 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Average loss at step 3400: 1.670272 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Average loss at step 3500: 1.652968 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Average loss at step 3600: 1.665230 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Average loss at step 3700: 1.637889 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Average loss at step 3800: 1.642433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Average loss at step 3900: 1.634907 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Average loss at step 4000: 1.651065 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "maine scholaphikus gaus slaxy orhami calaera aidalay predibrous induated the mar\n",
      "ho those earle ye alsoman a jeland on countrai ald or although with the revelts \n",
      "d actinaies the majaree also kremate in gians of meholary s tatias our preue mod\n",
      "xici knalian and ginamas three four zero  nover featalis sublications to the ata\n",
      "n the seatoral novincty dated doneatifitiunch oteredtly mampages somin generally\n",
      "================================================================================\n",
      "Average loss at step 4100: 1.626515 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Average loss at step 4200: 1.632529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Average loss at step 4300: 1.608888 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Average loss at step 4400: 1.607388 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Average loss at step 4500: 1.611116 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Average loss at step 4600: 1.613747 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Average loss at step 4700: 1.619148 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Average loss at step 4800: 1.628366 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Average loss at step 4900: 1.627488 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Average loss at step 5000: 1.603652 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "joche us his distant in there it he to proploder to saach witqost the alsower gi\n",
      "ston be there has his that the two nine two zero four five manted convident sppe\n",
      "treabace codvanted his many persond death or regitues from the insenticate is ga\n",
      "s was expetsing sug to plust to ide the two shop rethover reluging powersi or pr\n",
      "x dewithafus ethas for empitor jipainestr prociations for ind befend mouth with \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 5100: 1.595851 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Average loss at step 5200: 1.586696 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Average loss at step 5300: 1.571880 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Average loss at step 5400: 1.570442 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Average loss at step 5500: 1.558164 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Average loss at step 5600: 1.574508 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Average loss at step 5700: 1.568900 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Average loss at step 5800: 1.574063 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Average loss at step 5900: 1.570663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Average loss at step 6000: 1.542583 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      " divine solimions to denerwe nationary part the lems york ratall by presendulate\n",
      "y villes in a was caplish on invosed its the centrally over only in maguida one \n",
      "hit their in the fience is the uperian work and modable or plaiminus or other se\n",
      "er emotil of parpany anotive comproved and lour in thesen broak imlimmable leatu\n",
      "ion as the sinzes in escopratific his forbaines teakon jay soppant to charty s w\n",
      "================================================================================\n",
      "Average loss at step 6100: 1.557246 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 6200: 1.529928 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Average loss at step 6300: 1.539527 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Average loss at step 6400: 1.537191 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Average loss at step 6500: 1.554116 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Average loss at step 6600: 1.590504 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Average loss at step 6700: 1.575992 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Average loss at step 6800: 1.599502 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Average loss at step 6900: 1.573255 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Average loss at step 7000: 1.565690 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "bod for quetwed the people for the seating war nanrams the cl anarx letter hola \n",
      "x closelinitt inniverted fineanics schropheranioc american montes where akences \n",
      "wing one eight two one nine two tive to curans new one one nine three six notesc\n",
      "man g not enel three five with atsectremike universion titurent of genare and in\n",
      "west ticon emperolly rechict fiest one five two gasies and serceded and economic\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8-*-#\n",
    "#from handleData import LoadData, characters\n",
    "#from lstm_model import LSTM_Cell\n",
    "#from BatchGenerator import BatchGenerator\n",
    "#from sample import sample, sample_distribution, random_distribution\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#from config import config\n",
    "\n",
    "def get_optimizer(loss):\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # 为了避免梯度爆炸的问题，我们求出梯度的二范数。\n",
    "    # 然后判断该二范数是否大于1.25，若大于，则变成\n",
    "    # gradients * (1.25 / global_norm)作为当前的gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    # 将刚刚求得的梯度组装成相应的梯度下降法\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    return optimizer, learning_rate\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    # 计算交叉熵\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "loadData = LoadData()\n",
    "train_text = loadData.train_text\n",
    "valid_text = loadData.valid_text\n",
    "\n",
    "train_batcher = BatchGenerator(text=train_text, batch_size=config.batch_size, num_unrollings=config.num_unrollings)\n",
    "vaild_batcher = BatchGenerator(text=valid_text, batch_size=1, num_unrollings=1)\n",
    "\n",
    "# 定义训练数据由num_unrollings个占位符组成\n",
    "train_data = list()\n",
    "for _ in range(config.num_unrollings + 1):\n",
    "    train_data.append(\n",
    "        tf.placeholder(tf.float32, shape=[config.batch_size, config.vocabulary_size]))\n",
    "\n",
    "train_input = train_data[:config.num_unrollings]\n",
    "train_label= train_data[1:]\n",
    "\n",
    "# define the lstm train model\n",
    "model = LSTM_Cell(\n",
    "    train_data=train_input,\n",
    "    train_label=train_label)\n",
    "# get the loss and the prediction\n",
    "logits, loss, train_prediction = model.loss_func()\n",
    "optimizer, learning_rate = get_optimizer(loss)\n",
    "\n",
    "# 定义样本(通过训练后的rnn网络自动生成文字)的输入,输出,重置\n",
    "sample_input = tf.placeholder(tf.float32, shape=[1, config.vocabulary_size])\n",
    "save_sample_output = tf.Variable(tf.zeros([1, config.num_nodes]))\n",
    "save_sample_state = tf.Variable(tf.zeros([1, config.num_nodes]))\n",
    "reset_sample_state = tf.group(\n",
    "    save_sample_output.assign(tf.zeros([1, config.num_nodes])),\n",
    "    save_sample_state.assign(tf.zeros([1, config.num_nodes])))\n",
    "\n",
    "sample_output, sample_state = model._run(\n",
    "    sample_input, save_sample_output, save_sample_state)\n",
    "with tf.control_dependencies([save_sample_output.assign(sample_output),\n",
    "                                save_sample_state.assign(sample_state)]):\n",
    "    # 生成样本\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, model.w, model.b))\n",
    "\n",
    "# training\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized....\")\n",
    "    mean_loss = 0\n",
    "    for step in range(config.num_steps):\n",
    "        batches = train_batcher.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(config.num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        # 计算每一批数据的平均损失\n",
    "        mean_loss += l\n",
    "        if step % config.summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / config.summary_frequency\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (config.summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            reset_sample_state.run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
